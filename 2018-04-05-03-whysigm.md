# Why sigmoid?

Why do we need to put the sigmoid function in every neurons in a neural network? Why not just weighted sum plus bias? Why do we need to normalize the value into [0,1]?

I then read a stackexchange ask which leads me to [a list of activation functions](flst), where I get names of two friends I frequently met:
- Identity
- Step

[flst]: stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons
